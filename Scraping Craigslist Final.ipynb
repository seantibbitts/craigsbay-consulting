{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If scraping all of California, pull sites and regions from Craigslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_links_url = 'https://geo.craigslist.org/iso/us/ca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_links_page = requests.get(ca_links_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(ca_links_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geo_site_list = soup.find('ul', class_='geo-site-list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = geo_site_list.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [li.find('a')['href'] for li in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract regions from links\n",
    "import re\n",
    "regions = [re.findall(r\"[\\w']+\", link)[1] for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only scraping four regions, though, so hard-code those in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://bakersfield.craigslist.org','https://sacramento.craigslist.org','https://redding.craigslist.org',\n",
    "         'https://sandiego.craigslist.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['bakersfield','sacramento','redding','sandiego']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-code category codes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['ata','ara','sna','pta','ava','baa','haa','bip','bia','bpa','boo','bka','ema','moa','cla','syp','sya','fua',\n",
    "        'hva','jwa','mpa','mca','msa','pha','rva','tla','taa','tra','vga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = requests.utils.default_headers()\n",
    "headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account for possible dropped connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting constant dropped connections\n",
    "# Solution from this site: https://www.peterbe.com/plog/best-practice-with-retries-with-requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main scraping code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "# Iterate through links and regions simultaneously\n",
    "for base_url, region in zip(links, regions):\n",
    "    print(f'Region: {region}')\n",
    "    # Iterate through categories\n",
    "    for cat in cats:\n",
    "        print(f'\\tCategory: {cat}')\n",
    "        this_results_page_num = 0\n",
    "        counter = 0\n",
    "        visit_next = True\n",
    "        # Iterate through results pages\n",
    "        while visit_next:\n",
    "            print(f'\\t\\tThis results page num: {this_results_page_num}')\n",
    "            if this_results_page_num > 0:\n",
    "                next_results_page_link = base_url + '/search/' + cat + '?s=' + str(this_results_page_num)\n",
    "            else:\n",
    "                next_results_page_link = base_url + '/search/' + cat\n",
    "            response = requests_retry_session().get(next_results_page_link, headers = headers)\n",
    "            results_page = BeautifulSoup(response.text, 'html.parser')\n",
    "            results = results_page.find_all('li', class_ = 'result-row')\n",
    "            if not results:\n",
    "                print('\\t\\tNo results!')\n",
    "                visit_next = False\n",
    "                continue\n",
    "            result_anchors = [result.find('a', class_ = 'result-title') for result in results]\n",
    "            result_links = [anchor.get('href') for anchor in result_anchors]\n",
    "            result_ids = [result.get('data-pid') for result in results]\n",
    "            if response.status_code != 200:\n",
    "                warn('\\t\\tRequest: {}; Status code: {}'.format(requests, response.status_code))\n",
    "                visit_next = False\n",
    "                continue\n",
    "            # Iterate through results in results page\n",
    "            for link in result_links:\n",
    "                post_id = re.findall(r\"/([0-9]+)\\.html\", link)[0]\n",
    "                this_row = {}\n",
    "                counter += 1\n",
    "                # Sleep between 1/20 and 1/4 of a second\n",
    "                sleep(randint(1,5)/20)\n",
    "                response = requests_retry_session().get(link, headers = headers)\n",
    "                if response.status_code != 200:\n",
    "                    warn('\\t\\t\\tRequest: {}; Status code: {}'.format(requests, response.status_code))\n",
    "                    continue\n",
    "                result = BeautifulSoup(response.text, 'html.parser')\n",
    "                crumb = result.find('li', class_ = 'crumb category')\n",
    "                if crumb:\n",
    "                    category_anchor = crumb.find('a')\n",
    "                    if category_anchor:\n",
    "                        category = category_anchor.text\n",
    "                title_span = result.find('span', id='titletextonly')\n",
    "                if title_span:\n",
    "                    title = title_span.text\n",
    "                price_span = result.find('span', class_='price')\n",
    "                if price_span:\n",
    "                    price = price_span.text\n",
    "                # Get all sidebar attributes for each posting (if they exist)\n",
    "                these_uncat_attrs = []\n",
    "                these_attrs = {}\n",
    "                attr_para = result.find_all('p', class_='attrgroup')\n",
    "                if attr_para:\n",
    "                    for para in attr_para:\n",
    "                        attr_spans = para.find_all('span')\n",
    "                        for span in attr_spans:\n",
    "                            if not ':' in span.text:\n",
    "                                b = span.find('b')\n",
    "                                if b:\n",
    "                                    these_uncat_attrs.append(b.text)\n",
    "                            else:\n",
    "                                key = span.text.split(':')[0]\n",
    "                                value = span.find('b').text\n",
    "                                these_attrs[key] = value\n",
    "                # Get location data from map\n",
    "                loc_div = result.find('div', id='map')\n",
    "                if loc_div: \n",
    "                    lat = loc_div.get('data-latitude')\n",
    "                    lon = loc_div.get('data-longitude')\n",
    "                else:\n",
    "                    lat = None\n",
    "                    lon = None\n",
    "                # Assign each piece of data to its key in the dictionary\n",
    "                if post_id:\n",
    "                    this_row['post_id'] = post_id\n",
    "                if category:\n",
    "                    this_row['category'] = category\n",
    "                if title:\n",
    "                    this_row['title'] = title\n",
    "                if price:\n",
    "                    this_row['price'] = price\n",
    "                this_row['uncat_attrs'] = these_uncat_attrs\n",
    "                this_row['attrs'] = these_attrs\n",
    "                this_row['latitude'] = lat\n",
    "                this_row['longitude'] = lon\n",
    "                this_row['region'] = region\n",
    "                this_row['url'] = link\n",
    "                # Append dictionary to 'rows' variable\n",
    "                rows.append(this_row)\n",
    "                print(f'\\t\\t\\t{counter}')\n",
    "                print('\\t\\t\\tDidn\\'t pass!')\n",
    "                print('\\t\\t\\tWrote data!')\n",
    "            this_results_page_num += 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract condition (if exists) from 'attrs' dictionary column\n",
    "data['condition'] = data['attrs'].map(lambda x: x.get('condition'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data/all_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
